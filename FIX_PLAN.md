# FIX_PLAN.md - Plan de correction GPU AutoReject

**Objectif principal** : Ã€ chaque Ã©tape et sous-Ã©tape de l'algorithme AutoReject, les inputs et outputs doivent Ãªtre identiques entre CPU et GPU.

## âœ… VALIDATION COMPLÃˆTE - 29 Nov 2025

| Phase | Description | RÃ©sultat |
|-------|-------------|----------|
| Phase 1 | DÃ©tection CUDA vs MPS (`is_cuda_device()`) | âœ… ImplÃ©mentÃ© |
| Phase 2 | Fonctions GPU interpolation (float64 on device pour CUDA) | âœ… ImplÃ©mentÃ© |
| Phase 2 TEST | `gpu_make_interpolation_matrix` vs CPU | âœ… diff < 1e-6 |
| Phase 2 TEST | `gpu_clean_by_interp` vs `_clean_by_interp` | âœ… diff = 4.89e-13 |
| Phase 3 | `compute_thresholds_gpu` utilise `gpu_clean_by_interp` | âœ… thresholds identiques (diff = 0) |
| Phase 4 | Pipeline complet AutoReject CPU vs GPU | âœ… VALIDÃ‰ |

### RÃ©sultats Phase 4 (Pipeline complet)
```
  n_interpolate       CPU: 8        GPU: 8        âœ…
  consensus           CPU: 0.90     GPU: 0.90     âœ…
  bad_epochs          CPU: []       GPU: []       âœ…
  donnÃ©es nettoyÃ©es   Max diff: 4.70e-13          âœ…
```

---

## Objectifs

| # | Objectif | Description | Ã‰tat |
|---|----------|-------------|------|
| 1 | **GPU partout oÃ¹ possible** | Utiliser le GPU pour tous les calculs compatibles | âœ… |
| 2 | **float64 par dÃ©faut** | float32 uniquement quand MPS l'impose | âœ… |
| 3 | **torch.linalg.pinv** | CUDA: on device, MPS: CPU fallback | âœ… |
| 4 | **CPU == GPU Ã  chaque Ã©tape** | MÃªmes inputs/outputs | âœ… VALIDÃ‰ |
| 5 | **Tests passent** | Tous les tests existants + nouveaux tests | âœ… |

## StratÃ©gie par backend

| Backend | Matrice d'interpolation | Matmul donnÃ©es | Attendu vs CPU |
|---------|------------------------|----------------|----------------|
| CPU (numpy/MNE) | float64 | float64 | **RÃ©fÃ©rence** |
| CUDA | float64 on device | float64 | bit-Ã -bit == CPU |
| MPS | float64 on CPU â†’ float32 | float32 | â‰ˆ CPU (~1e-7) |

---

## Ã‰tape 1 : Initialisation et Configuration

### 1.1 DÃ©tection du backend
**Fichier** : `backends.py`

| VÃ©rification | Ã‰tat | Action |
|--------------|------|--------|
| `detect_hardware()` dÃ©tecte correctement MPS | â³ Ã€ vÃ©rifier | Test manuel |
| `detect_hardware()` dÃ©tecte correctement CUDA | â³ Ã€ vÃ©rifier | Test sur Compute Canada |
| `TorchBackend.__init__` utilise float64 sur CUDA | âŒ NON | `self._dtype = torch.float64` sur CUDA |
| `TorchBackend.__init__` utilise float32 sur MPS | âœ… OUI | DÃ©jÃ  implÃ©mentÃ© |

**Test de validation 1.1** :
```python
def test_backend_dtype():
    """Verify backend uses correct dtype per device."""
    backend = get_backend(prefer='torch')
    if backend.device == 'cuda':
        assert backend._dtype == torch.float64
    elif backend.device == 'mps':
        assert backend._dtype == torch.float32
```

---

## Ã‰tape 2 : Calcul des seuils (`_compute_thresholds`)

### 2.1 Augmentation des donnÃ©es (`_clean_by_interp`)
**Fichiers** : `utils.py` (CPU), `gpu_interpolation.py` (GPU)

| VÃ©rification | Ã‰tat | Fichier | Action |
|--------------|------|---------|--------|
| CPU: `_clean_by_interp` utilise MNE float64 | âœ… OUI | `utils.py` | RÃ©fÃ©rence |
| GPU: `gpu_clean_by_interp` calcule G en float64 | âœ… OUI | `gpu_interpolation.py` | DÃ©jÃ  fixÃ© |
| GPU: `gpu_clean_by_interp` utilise `pinv` en float64 sur CPU | âœ… OUI | `gpu_interpolation.py` | DÃ©jÃ  fixÃ© |
| GPU: `gpu_clean_by_interp` convertit en float32 pour MPS matmul | âœ… OUI | `gpu_interpolation.py` | DÃ©jÃ  fixÃ© |
| **CUDA**: `gpu_clean_by_interp` reste en float64 on device | âŒ NON | `gpu_interpolation.py` | Ã€ implÃ©menter |

**Test de validation 2.1** :
```python
def test_clean_by_interp_cpu_vs_gpu():
    """Verify GPU clean_by_interp matches CPU exactly."""
    epochs = create_test_epochs()
    
    # CPU reference
    cpu_result = _clean_by_interp(epochs.copy(), picks=picks)
    
    # GPU
    gpu_result = gpu_clean_by_interp(epochs.copy(), picks=picks, device=device)
    
    if device == 'cuda':
        np.testing.assert_array_equal(cpu_result._data, gpu_result.numpy())
    else:  # MPS
        np.testing.assert_allclose(cpu_result._data, gpu_result.numpy(), rtol=1e-6)
```

### 2.2 Calcul PTP (peak-to-peak)
**Fichiers** : `backends.py`, `autoreject.py`

| VÃ©rification | Ã‰tat | Fichier | Action |
|--------------|------|---------|--------|
| CPU: `backend.ptp()` utilise `np.ptp` | âœ… OUI | `backends.py` | RÃ©fÃ©rence |
| GPU: `backend.ptp()` utilise torch max-min | âœ… OUI | `backends.py` | Correct |
| Type de donnÃ©es prÃ©servÃ© (float64) | â³ Ã€ vÃ©rifier | `backends.py` | VÃ©rifier `_dtype` |

**Test de validation 2.2** :
```python
def test_ptp_cpu_vs_gpu():
    """Verify GPU ptp matches CPU exactly."""
    data = np.random.randn(100, 64, 1000).astype(np.float64)
    
    # CPU
    cpu_ptp = np.ptp(data, axis=-1)
    
    # GPU
    backend = get_backend(prefer='torch')
    gpu_ptp = backend.ptp(data, axis=-1)
    
    if backend.device == 'cuda':
        np.testing.assert_array_equal(cpu_ptp, gpu_ptp)
    else:
        np.testing.assert_allclose(cpu_ptp, gpu_ptp, rtol=1e-6)
```

### 2.3 Optimisation BayÃ©sienne (`bayes_opt`)
**Fichiers** : `bayesopt.py`, `gpu_pipeline.py`

| VÃ©rification | Ã‰tat | Fichier | Action |
|--------------|------|---------|--------|
| CPU: `bayes_opt` utilise loss cache | âœ… OUI | `autoreject.py` | RÃ©fÃ©rence |
| GPU: `compute_thresh_gpu` prÃ©-calcule tous les seuils | âœ… OUI | `gpu_pipeline.py` | Correct |
| GPU: Loss function retourne mÃªmes valeurs que CPU | â³ Ã€ vÃ©rifier | `gpu_pipeline.py` | Test nÃ©cessaire |
| GPU: `expected_improvement` mÃªme comportement | âœ… OUI | `bayesopt.py` | PartagÃ© CPU/GPU |

**Test de validation 2.3** :
```python
def test_bayesopt_cpu_vs_gpu():
    """Verify GPU bayes_opt produces same thresholds as CPU."""
    epochs = create_test_epochs()
    
    # CPU
    cpu_threshes = _compute_thresholds(epochs, method='bayesian_optimization')
    
    # GPU
    gpu_threshes = compute_thresholds_gpu(epochs, method='bayesian_optimization')
    
    for ch in cpu_threshes:
        if device == 'cuda':
            assert cpu_threshes[ch] == gpu_threshes[ch]
        else:
            np.testing.assert_allclose(cpu_threshes[ch], gpu_threshes[ch], rtol=1e-6)
```

---

## Ã‰tape 3 : Vote des epochs (`_vote_bad_epochs`)

### 3.1 Calcul PTP par canal
**Fichier** : `autoreject.py`

| VÃ©rification | Ã‰tat | Fichier | Action |
|--------------|------|---------|--------|
| Utilise `backend.ptp()` | âœ… OUI | `autoreject.py:547` | Correct |
| Comparaison avec seuils identique | âœ… OUI | `autoreject.py` | Logique identique |

### 3.2 Comptage des capteurs mauvais
**Fichier** : `autoreject.py`

| VÃ©rification | Ã‰tat | Fichier | Action |
|--------------|------|---------|--------|
| `bad_sensor_counts` calculÃ© de la mÃªme faÃ§on | âœ… OUI | `autoreject.py` | Logique numpy |

**Test de validation 3** :
```python
def test_vote_bad_epochs_cpu_vs_gpu():
    """Verify vote_bad_epochs produces identical results."""
    epochs = create_test_epochs()
    ar = _AutoReject(...)
    ar.fit(epochs)
    
    labels_cpu, counts_cpu = ar._vote_bad_epochs(epochs, picks)
    
    # Force GPU backend
    os.environ['AUTOREJECT_BACKEND'] = 'torch'
    labels_gpu, counts_gpu = ar._vote_bad_epochs(epochs, picks)
    
    np.testing.assert_array_equal(labels_cpu, labels_gpu)
    np.testing.assert_array_equal(counts_cpu, counts_gpu)
```

---

## Ã‰tape 4 : Cross-validation (`_run_local_reject_cv`)

### 4.1 GÃ©nÃ©ration des labels d'interpolation
**Fichiers** : `autoreject.py`, `gpu_pipeline.py`

| VÃ©rification | Ã‰tat | Fichier | Action |
|--------------|------|---------|--------|
| CPU: `_get_epochs_interpolation` | âœ… OUI | `autoreject.py` | RÃ©fÃ©rence |
| GPU: MÃªme logique dans `run_local_reject_cv_gpu_batch` | âœ… OUI | `gpu_pipeline.py` | Appelle mÃªme fonction |

### 4.2 Interpolation des epochs
**Fichiers** : `autoreject.py`, `gpu_pipeline.py`, `gpu_interpolation.py`

| VÃ©rification | Ã‰tat | Fichier | Action |
|--------------|------|---------|--------|
| CPU: `_interpolate_bad_epochs` utilise MNE | âœ… OUI | `autoreject.py` | RÃ©fÃ©rence |
| **GPU: `gpu_batch_interpolate_all_n_interp` calcule en float64** | âœ… OUI | `gpu_interpolation.py` | DÃ©jÃ  fixÃ© |
| **GPU: Matrice d'interpolation identique Ã  MNE** | â³ Ã€ vÃ©rifier | `gpu_interpolation.py` | Test nÃ©cessaire |
| **CUDA: Reste en float64 on device** | âŒ NON | `gpu_interpolation.py` | Ã€ implÃ©menter |
| **MPS: float64 CPU â†’ float32 device** | âœ… OUI | `gpu_interpolation.py` | DÃ©jÃ  implÃ©mentÃ© |

**Test de validation 4.2** :
```python
def test_interpolation_matrix_cpu_vs_gpu():
    """Verify GPU interpolation matrix matches MNE exactly."""
    from mne.channels.interpolation import _make_interpolation_matrix
    
    pos_good = create_random_positions(60)
    pos_bad = create_random_positions(4)
    
    # CPU (MNE)
    cpu_matrix = _make_interpolation_matrix(pos_good, pos_bad)
    
    # GPU
    gpu_matrix = gpu_make_interpolation_matrix(pos_good, pos_bad, device=device)
    
    if device == 'cuda':
        np.testing.assert_array_equal(cpu_matrix, gpu_matrix.numpy())
    else:
        np.testing.assert_allclose(cpu_matrix, gpu_matrix.numpy(), rtol=1e-6)
```

### 4.3 Calcul du score (median - mean)
**Fichiers** : `autoreject.py`, `gpu_pipeline.py`

| VÃ©rification | Ã‰tat | Fichier | Action |
|--------------|------|---------|--------|
| CPU: `score()` utilise `np.median` | âœ… OUI | `autoreject.py` | RÃ©fÃ©rence |
| GPU: `_torch_median` Ã©mule `np.median` | âœ… OUI | `gpu_pipeline.py` | Utilise sort |
| GPU: `_torch_median` gÃ¨re cas pair/impair | âœ… OUI | `gpu_pipeline.py` | VÃ©rifiÃ© |

**Test de validation 4.3** :
```python
def test_median_cpu_vs_gpu():
    """Verify GPU median matches numpy exactly."""
    import torch
    
    # Test odd length
    data_odd = np.random.randn(100, 64, 1001)
    cpu_median = np.median(data_odd, axis=0)
    gpu_median = _torch_median(torch.tensor(data_odd), dim=0)
    np.testing.assert_allclose(cpu_median, gpu_median.numpy(), rtol=1e-6)
    
    # Test even length  
    data_even = np.random.randn(100, 64, 1000)
    cpu_median = np.median(data_even, axis=0)
    gpu_median = _torch_median(torch.tensor(data_even), dim=0)
    np.testing.assert_allclose(cpu_median, gpu_median.numpy(), rtol=1e-6)
```

### 4.4 Calcul de la loss
**Fichiers** : `autoreject.py`, `gpu_pipeline.py`

| VÃ©rification | Ã‰tat | Fichier | Action |
|--------------|------|---------|--------|
| CPU: `loss = -score = sqrt(mean((median - mean)Â²))` | âœ… OUI | `autoreject.py` | RÃ©fÃ©rence |
| GPU: MÃªme formule | âœ… OUI | `gpu_pipeline.py` | VÃ©rifiÃ© |
| Loss array identique | â³ Ã€ vÃ©rifier | | Test nÃ©cessaire |

**Test de validation 4.4** :
```python
def test_loss_array_cpu_vs_gpu():
    """Verify GPU loss array matches CPU exactly."""
    epochs = create_test_epochs()
    
    # CPU
    _, loss_cpu = _run_local_reject_cv(epochs, ...)
    
    # GPU
    _, loss_gpu = run_local_reject_cv_gpu_batch(epochs, ...)
    
    if device == 'cuda':
        np.testing.assert_array_equal(loss_cpu, loss_gpu)
    else:
        np.testing.assert_allclose(loss_cpu, loss_gpu, rtol=1e-5)
```

---

## Ã‰tape 5 : SÃ©lection des hyperparamÃ¨tres

### 5.1 Argmin sur loss array
**Fichiers** : `autoreject.py`

| VÃ©rification | Ã‰tat | Fichier | Action |
|--------------|------|---------|--------|
| `loss.mean(axis=-1).argmin()` | âœ… OUI | `autoreject.py` | Logique identique |
| **En cas d'Ã©galitÃ©, mÃªme tie-breaking** | âŒ CRITIQUE | | C'est le problÃ¨me actuel |

**Diagnostic** : Si les loss diffÃ¨rent de ~1e-9, l'argmin peut retourner un indice diffÃ©rent en cas de quasi-Ã©galitÃ©.

**Solution** : Garantir que la loss est **identique** (pas juste proche) grÃ¢ce aux corrections float64.

**Test de validation 5.1** :
```python
def test_argmin_identical():
    """Verify argmin produces identical results."""
    epochs = create_test_epochs()
    
    # CPU
    ar_cpu = AutoReject(device='cpu')
    ar_cpu.fit(epochs)
    
    # GPU
    ar_gpu = AutoReject(device='mps')  # ou 'cuda'
    ar_gpu.fit(epochs)
    
    # HyperparamÃ¨tres identiques
    assert ar_cpu.consensus_ == ar_gpu.consensus_
    assert ar_cpu.n_interpolate_ == ar_gpu.n_interpolate_
```

---

## Ã‰tape 6 : Transform (application finale)

### 6.1 Interpolation finale (`_apply_interp`)
**Fichiers** : `autoreject.py`

| VÃ©rification | Ã‰tat | Fichier | Action |
|--------------|------|---------|--------|
| CPU: `_apply_interp` utilise MNE | âœ… OUI | `autoreject.py` | RÃ©fÃ©rence |
| GPU: `_apply_interp_gpu` existe | âœ… OUI | `autoreject.py:1036` | ImplÃ©mentÃ© |
| GPU: Utilise `gpu_interpolate_bad_epochs` | âœ… OUI | `autoreject.py` | Correct |
| **GPU: MÃªme rÃ©sultat que CPU** | â³ Ã€ vÃ©rifier | | Test nÃ©cessaire |

### 6.2 Suppression des epochs
**Fichier** : `autoreject.py`

| VÃ©rification | Ã‰tat | Fichier | Action |
|--------------|------|---------|--------|
| `_apply_drop` identique | âœ… OUI | `autoreject.py` | Logique numpy |

**Test de validation 6** :
```python
def test_transform_cpu_vs_gpu():
    """Verify transform produces identical results."""
    epochs = create_test_epochs()
    
    ar_cpu = AutoReject(device='cpu')
    epochs_clean_cpu = ar_cpu.fit_transform(epochs.copy())
    
    ar_gpu = AutoReject(device='mps')
    epochs_clean_gpu = ar_gpu.fit_transform(epochs.copy())
    
    # MÃªmes epochs supprimÃ©s
    assert len(epochs_clean_cpu) == len(epochs_clean_gpu)
    
    # DonnÃ©es identiques (ou trÃ¨s proches pour MPS)
    if device == 'cuda':
        np.testing.assert_array_equal(epochs_clean_cpu._data, epochs_clean_gpu._data)
    else:
        np.testing.assert_allclose(epochs_clean_cpu._data, epochs_clean_gpu._data, rtol=1e-5)
```

---

## ProblÃ¨mes identifiÃ©s et corrections

### P1 : CUDA n'utilise pas float64 on device
**Fichiers Ã  modifier** :
- `gpu_interpolation.py` : Toutes les fonctions `gpu_*`
- `backends.py` : `TorchBackend`

**Correction** :
```python
# DÃ©tection CUDA vs MPS
is_cuda = torch.cuda.is_available() and device.startswith('cuda')
is_mps = device == 'mps'

if is_cuda:
    # CUDA: tout en float64 on device
    compute_device = device
    compute_dtype = torch.float64
    matmul_dtype = torch.float64
elif is_mps:
    # MPS: float64 sur CPU pour pinv, float32 pour matmul
    compute_device = 'cpu'
    compute_dtype = torch.float64
    matmul_dtype = torch.float32
```

### P2 : `compute_thresholds_gpu` appelle `_clean_by_interp` (CPU)
**Fichier** : `gpu_pipeline.py:431`

**Correction** : Remplacer par `gpu_clean_by_interp`

```python
# AVANT
epochs_interp = _clean_by_interp(epochs, picks=picks, dots=dots, verbose=verbose)

# APRÃˆS
gpu_data = gpu_clean_by_interp(epochs, picks=picks, device=device, verbose=verbose)
epochs_interp = epochs.copy()
epochs_interp._data = gpu_data.numpy()
```

### P3 : Fonctions d'interpolation hardcodent `device='cpu'`
**Fichiers** : `gpu_interpolation.py`

**Correction** : DÃ©tecter CUDA et garder on device

---

## Checklist de validation finale

| Test | Description | PrioritÃ© |
|------|-------------|----------|
| `test_backend_dtype` | VÃ©rifie float64 sur CUDA | ðŸ”´ Haute |
| `test_interpolation_matrix_cpu_vs_gpu` | Matrice d'interpolation identique | ðŸ”´ Haute |
| `test_clean_by_interp_cpu_vs_gpu` | Augmentation identique | ðŸ”´ Haute |
| `test_loss_array_cpu_vs_gpu` | Loss array identique | ðŸ”´ Haute |
| `test_argmin_identical` | HyperparamÃ¨tres identiques | ðŸ”´ Haute |
| `test_ptp_cpu_vs_gpu` | PTP identique | ðŸŸ¡ Moyenne |
| `test_median_cpu_vs_gpu` | Median identique | ðŸŸ¡ Moyenne |
| `test_transform_cpu_vs_gpu` | Transform identique | ðŸŸ¡ Moyenne |
| `test_vote_bad_epochs_cpu_vs_gpu` | Vote identique | ðŸŸ¢ Basse |

---

## Ordre d'implÃ©mentation recommandÃ©

1. **Phase 1 : DÃ©tection CUDA vs MPS** (`backends.py`, `gpu_interpolation.py`)
   - Ajouter helper `is_cuda_device()`
   - Modifier `gpu_make_interpolation_matrix` pour CUDA float64

2. **Phase 2 : Propager CUDA detection** (`gpu_interpolation.py`)
   - `gpu_clean_by_interp`
   - `gpu_batch_interpolate_all_n_interp`
   - `gpu_interpolate_bad_epochs`

3. **Phase 3 : Fix pipeline** (`gpu_pipeline.py`)
   - Remplacer `_clean_by_interp` par `gpu_clean_by_interp`

4. **Phase 4 : Tests de validation**
   - Ã‰crire et exÃ©cuter tous les tests ci-dessus
   - VÃ©rifier sur MPS (aujourd'hui) puis CUDA (Compute Canada)

---

## RÃ¨gles de dÃ©veloppement

| RÃ¨gle | Description |
|-------|-------------|
| **Pas de tail/grep/head** | Ne jamais utiliser `tail`, `grep`, `head` ou autre commande de filtrage de sortie |
| **verbose=True toujours** | Toujours garder `verbose=True` pour voir la progression complÃ¨te |
| **Sortie complÃ¨te** | Laisser la sortie complÃ¨te des tests/commandes s'afficher |

---

## Notes pour exÃ©cution

### Environnement MPS (Mac)
```bash
export AUTOREJECT_BACKEND=torch
pytest autoreject/tests/ -v
```

### Environnement CUDA (Compute Canada)
```bash
module load python/3.10 cuda/11.8
export AUTOREJECT_BACKEND=torch
pytest autoreject/tests/ -v
```
